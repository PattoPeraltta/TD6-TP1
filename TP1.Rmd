---
output:
  html_document: default
  pdf_document: default
---
```{r message=FALSE, warning=FALSE}
# Librerias
library(readr)
library(dplyr)
library(ggplot2)
library(pROC)
library(caret)
library(rpart)

# Semilla
set.seed(20250818)
```

## Sección 1: Introducción al problema

### Descripción del conjunto de datos

El conjunto de datos seleccionado corresponde a la **campaña de marketing de una institución bancaria de Portugal**, publicado en el *UCI Machine Learning Repository*. Contiene **45.211 observaciones** y **17 variables predictoras**, que incluyen tanto atributos **numéricos** (edad, duración de la última llamada, balance de la cuenta, entre otros) como **categóricos** (profesión, estado civil, nivel educativo, tipo de vivienda, canal de contacto, mes de contacto, etc.).

La variable objetivo es **binaria**: indica si el cliente **aceptó (yes)** o **rechazó (no)** suscribirse a un depósito a plazo luego de la campaña de marketing. Este problema se formula como una tarea de **clasificación binaria**, donde el objetivo es predecir la propensión del cliente a aceptar la oferta bancaria en base a sus características sociodemográficas y al historial de interacciones con el banco.

### Justificación para el uso de árboles de decisión

La elección de este conjunto de datos es especialmente adecuada para aplicar **árboles de decisión** y métodos derivados (Random Forests, Gradient Boosted Trees), debido a:

1.  **Mezcla de variables**: los árboles pueden manejar de manera natural tanto predictores numéricos como categóricos sin requerir transformaciones complejas.
2.  **Relaciones no lineales y reglas complejas**: la decisión de un cliente no depende de una regla simple (ej., "si edad \> 30, entonces sí"), sino de la interacción entre múltiples factores. Los árboles permiten modelar estas interacciones mediante divisiones jerárquicas.
3.  **Interpretabilidad**: en un contexto bancario, es valioso explicar por qué un modelo predice que un cliente aceptará o no la campaña. Los árboles permiten obtener reglas claras y comprensibles.
4.  **Escalabilidad**: el dataset tiene un tamaño lo suficientemente grande para evaluar el desempeño de modelos complejos, pero no excesivo como para impedir un train eficiente.

En resumen, este conjunto de datos no solo plantea un problema realista e importante de clasificación binaria, sino que también se ajusta adecuadamente a las capacidades de los árboles de decisión, permitiendo explorar tanto sus poderes predictivo y capacidades interpretativas.

## Sección 2: Preparación de los datos

### Carga del dataset

```{r message=FALSE, warning=FALSE}
# Cargar dataset (suponiendo que el archivo se llama bank.csv)
bank <- read.csv("bank.csv", sep = ";")

# Ver estructura inicial
str(bank)

# Variable objetivo (binaria: yes/no)
table(bank$y)
```

### Analisis exploratorio

```{r message=FALSE, warning=FALSE}
# Resumen general de variables numéricas
summary(select_if(bank, is.numeric))

# Frecuencias de algunas variables categóricas clave
table(bank$job)
table(bank$education)

# Distribución de la edad de los clientes (grafico)
ggplot(bank, aes(x = age)) +
  geom_histogram(binwidth = 5, fill = "steelblue", color = "white") +
  labs(x = "Edad", y = "Frecuencia")

# Comparación de una variable categórica con la variable objetivo
ggplot(bank, aes(x = job, fill = y)) +
  geom_bar(position = "fill") +
  labs(
    title = "Proporción de aceptación del depósito según ocupación",
    x = "Ocupación", y = "Proporción"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

### Comentarios sobre los datos:

-   **Edad**: la mayoría de los clientes se concentran entre los 30 y 50 años, con algunos outliers en edades más altas.

-   **Ocupación**: existen diferencias notorias en la tasa de aceptación de la campaña según el tipo de trabajo; por ejemplo, clientes en sectores administrativos tienden a tener una proporción distinta a los desempleados o estudiantes.

-   **Balance de cuenta y duración de llamada**: presentan gran variabilidad y podrían ser predictores relevantes para el modelo.

-   **Desbalance de clases**: la variable objetivo está desbalanceada (muchos más *no* que *sí*), lo cual será importante a considerar en la modelización.

## Sección 3: Construcción de un árbol de decisión básico

```{r message=FALSE, warning=FALSE}
n <- nrow(bank)

# límites
max_valid <- floor(0.15 * n)
max_test <- floor(0.15 * n)

# listas de índices
idx_entrenamiento <- integer(0)
idx_validacion <- integer(0)
idx_testeo <- integer(0)

for (i in 1:n) {
  r <- runif(1) # número aleatorio entre 0 y 1

  if (r < 0.15 && length(idx_validacion) < max_valid) {
    idx_validacion <- c(idx_validacion, i)
  } else if (r < 0.30 && length(idx_testeo) < max_test) {
    idx_testeo <- c(idx_testeo, i)
  } else {
    idx_entrenamiento <- c(idx_entrenamiento, i)
  }
}

# subconjuntos finales
entrenamiento <- bank[idx_entrenamiento, , drop = FALSE]
validacion <- bank[idx_validacion, , drop = FALSE]
testeo <- bank[idx_testeo, , drop = FALSE]
```

```{r message=FALSE, warning=FALSE}
# Entrenar árbol básico con defaults
arbol_basico <- rpart(
  y ~ .,
  data   = entrenamiento,
  method = "class"
)
```

### Hiperparámetros por defecto de `rpart`

-   **`minsplit = 20`**\
    Un nodo debe tener **al menos 20** observaciones para que se intente un split. Limita la fragmentación temprana y reduce el sobreajuste en nodos muy chicos.

-   **`minbucket = round(minsplit/3)` → \~7**\
    Tamaño mínimo de **cada hoja**. Con el valor por defecto de `minsplit`, cada hoja terminal debe tener \~7 observaciones. Evita hojas ínfimas e inestables.

-   **`cp = 0.01`** (complexity parameter)\
    Solo se aceptan splits que reduzcan el error relativo **≥ 1%**. Favorece árboles más chicos y acelera el ajuste. Luego se genera la secuencia de podas según `cp`.

-   **`xval = 10`**\
    **Validación cruzada 10-fold interna** para estimar el error y elegir el nivel de poda (en la tabla de `cp`).

-   **`maxdepth = 30`**\
    Profundidad máxima muy alta (prácticamente sin tope en la práctica). El control real de complejidad lo imponen `cp`, `minsplit` y `minbucket`.

- **`maxcompete = 4`**  
  Guarda hasta **4 splits competidores** (formas alternativas de hacer un split) por nodo (no afecta el ajuste; es metadato útil para inspección).

-   **`maxsurrogate = 5`**\
    Guarda hasta **5 variables sustitutas** por nodo para manejar **valores faltantes**.

-   **`usesurrogate = 2`**\
    Si falta la variable del split principal, **usa surrogates en orden**; si tampoco están disponibles, envía el caso por la **rama mayoritaria**.

-   **`surrogatestyle = 0`**\
    El “mejor” surrogate se elige por **número total de aciertos** (penaliza variables con muchos NA).

- (Clasificación) 
  - **split = "gini"**: Los cortes minimizan la impureza Gini
  - **prior**: Por defecto, proporcionales a las frecuencias de clase observadas en el dataset
  - **loss**: Matriz de costos uniforme (0 en la diagonal, 1 fuera de la diagonal)

```{r message=FALSE, warning=FALSE}
library(rpart.plot)
# Visualización
rpart.plot(arbol_basico, type = 2, extra = 106, box.palette = "GnBu")
```

**Altura observada:** 7 niveles.

1)  **Raíz — `duration < 477 s`**\
    Divide “llamadas cortas” vs “largas”. Es el corte más informativo: las cortas concentran **NO**, las largas elevan la chance de **YES**.

2)  **Rama izquierda (`duration < 477`)**

    -   Siguientes cortes: **`poutcome`** (resultados previos de campaña), **`month`** y umbrales más bajos de **`duration`** (≈ 180 s).\
    -   Interpretación: **llamadas muy cortas** y **sin buen antecedente** tienden a **NO**. Los ajustes con `month` refinan pequeños subgrupos.

3)  **Rama derecha (`duration ≥ 477`)**

    -   Segundo umbral de **`duration`**: **≈ 765 s** separa **llamadas largas** (alta probabilidad de **YES**) de las **intermedias**.\
    -   En la franja intermedia, la decisión se afina con **`job`**, **`marital`**, y en menor medida **`month/day`**:
        -   Ocupaciones no profesionales y `marital = married` empujan a **NO**.\
        -   Ocupaciones profesionales/servicios y otros estados civiles elevan la chance de **YES**.

## Sección 4: Evaluación del árbol de decisión básico

```{r}
# Normalización de niveles en los tres splits
for (nm in c("entrenamiento", "validacion", "testeo")) {
  if (exists(nm)) {
    tmp <- get(nm)
    if (!is.factor(tmp$y)) tmp$y <- as.factor(tmp$y)
    if (all(c("no", "yes") %in% levels(tmp$y))) {
      tmp$y <- factor(tmp$y, levels = c("no", "yes"))
    }
    assign(nm, tmp)
  }
}

# Chequeos
stopifnot(exists("arbol_basico"), exists("testeo"))
stopifnot("y" %in% names(testeo))
stopifnot(is.factor(testeo$y))

# Predicciones en testeo
pred_class <- predict(arbol_basico, newdata = testeo, type = "class")
pred_class <- factor(pred_class, levels = levels(testeo$y))

pred_prob_mat <- predict(arbol_basico, newdata = testeo, type = "prob")
pos_col <- if ("yes" %in% colnames(pred_prob_mat)) "yes" else tail(colnames(pred_prob_mat), 1)
pred_prob <- pred_prob_mat[, pos_col]

# Matriz de confusión y métricas
matriz_conf <- caret::confusionMatrix(pred_class, testeo$y, positive = "yes")

accuracy <- matriz_conf$overall["Accuracy"]
precision <- matriz_conf$byClass["Precision"]
recall <- matriz_conf$byClass["Recall"]
f1 <- matriz_conf$byClass["F1"]

# --- Gráficos: Matriz de confusión (conteos y proporciones) ------------------

# Conteos
cm_tbl <- as.table(matriz_conf$table)
cm_df <- as.data.frame(cm_tbl)
names(cm_df) <- c("real", "pred", "freq")

ggplot(cm_df, aes(x = real, y = pred, fill = freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = freq), size = 4) +
  scale_fill_gradient(low = "lightblue", high = "steelblue") +
  labs(
    title = "Matriz de confusión (conteos)",
    x = "Valor real",
    y = "Predicción",
    fill = "Frecuencia"
  ) +
  theme_minimal()

# Proporciones por clase real
cm_prop <- dplyr::group_by(cm_df, real)
cm_prop <- dplyr::mutate(cm_prop, prop = freq / sum(freq))
cm_prop <- dplyr::ungroup(cm_prop)

ggplot(cm_prop, aes(x = real, y = pred, fill = prop)) +
  geom_tile(color = "white") +
  geom_text(aes(label = sprintf("%.1f%%", 100 * prop)), size = 4) +
  scale_fill_gradient(low = "lavender", high = "purple") +
  labs(
    title = "Matriz de confusión (proporciones por real)",
    x = "Valor real",
    y = "Predicción",
    fill = "Proporción"
  ) +
  theme_minimal()


# ROC y AUC
roc_curve <- pROC::roc(
  testeo$y,
  pred_prob,
  levels = c("no", "yes"),
  direction = "<"
)
auc_value <- pROC::auc(roc_curve)

# Tabla resumen
resumen <- data.frame(
  Metrica = c("Accuracy", "Precision", "Recall", "F1", "AUC-ROC"),
  Valor = c(
    as.numeric(accuracy),
    as.numeric(precision),
    as.numeric(recall),
    as.numeric(f1),
    as.numeric(auc_value)
  )
)
knitr::kable(resumen, digits = 4, caption = "Punto 4 — Métricas (Test)")


# Gráfico de la curva ROC usando ggplot2
library(ggplot2)

# Extraer coordenadas de la curva ROC
roc_coords <- coords(roc_curve, ret = "all")
roc_df <- data.frame(
  specificity = roc_coords$specificity,
  sensitivity = roc_coords$sensitivity
)

# Línea de referencia (random classifier)
reference_df <- data.frame(
  x = c(0, 1),
  y = c(0, 1)
)

ggplot() +
  geom_line(
    data = roc_df, aes(x = 1 - specificity, y = sensitivity),
    color = "steelblue", linewidth = 1
  ) +
  geom_line(
    data = reference_df, aes(x = x, y = y),
    color = "darkred", linetype = "dashed", linewidth = 0.8
  ) +
  labs(
    title = "Curva ROC — Árbol básico (Test)",
    x = "1 - Especificidad (FPR)",
    y = "Sensibilidad (TPR)",
    caption = paste("AUC =", round(auc_value, 4))
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5),
    plot.caption = element_text(hjust = 0.5, face = "bold")
  )
```

Se puede ver claramente que el árbol básico supera claramente al clasificador aleatorio, ya que ```AUC = 0.7857```

### Optimización del modelo

En esta sección implementaremos una búsqueda exhaustiva de hiperparámetros para optimizar el árbol de decisión. Como se especifica en las instrucciones, fijaremos `cp = 0` y `xval = 0` para permitir que los árboles alcancen su profundidad máxima según los valores de `maxdepth`, `minsplit` y `minbucket`.

```{r message=FALSE, warning=FALSE}
# Función para evaluar modelo usando AUC-ROC
evaluar_auc <- function(modelo, datos) {
  pred_prob <- predict(modelo, newdata = datos, type = "prob")
  pos_col <- if ("yes" %in% colnames(pred_prob)) "yes" else tail(colnames(pred_prob), 1)
  prob_pos <- pred_prob[, pos_col]

  roc_obj <- pROC::roc(datos$y, prob_pos, levels = c("no", "yes"), direction = "<")
  auc_val <- as.numeric(pROC::auc(roc_obj))
  return(auc_val)
}

# Grid de hiperparámetros para búsqueda exhaustiva
grid_hiperparametros <- expand.grid(
  maxdepth = c(3, 4, 5, 6, 7, 8, 10, 12, 15),
  minsplit = c(10, 20, 30, 50, 100, 200),
  minbucket = c(5, 10, 15, 20, 30, 50)
)

# Filtrar combinaciones válidas (minbucket <= minsplit/2)
grid_hiperparametros <- subset(grid_hiperparametros, minbucket <= floor(minsplit / 2))

# Resultados de la búsqueda
resultados_optimizacion <- data.frame(
  maxdepth = grid_hiperparametros$maxdepth,
  minsplit = grid_hiperparametros$minsplit,
  minbucket = grid_hiperparametros$minbucket,
  auc_validacion = NA_real_,
  stringsAsFactors = FALSE
)

# Búsqueda exhaustiva
for (i in 1:nrow(grid_hiperparametros)) {
  # Configuración de control con cp=0 y xval=0
  control <- rpart.control(
    cp = 0,
    xval = 0,
    maxdepth = grid_hiperparametros$maxdepth[i],
    minsplit = grid_hiperparametros$minsplit[i],
    minbucket = grid_hiperparametros$minbucket[i]
  )

  # Entrenar modelo
  modelo_temp <- rpart(
    y ~ .,
    data = entrenamiento,
    method = "class",
    control = control
  )

  # Evaluar en validación
  resultados_optimizacion$auc_validacion[i] <- evaluar_auc(modelo_temp, validacion)
}

# Encontrar mejor modelo
mejor_idx <- which.max(resultados_optimizacion$auc_validacion)
mejor_arbol <- rpart(
  y ~ .,
  data = entrenamiento,
  method = "class",
  control = rpart.control(
    cp = 0,
    xval = 0,
    maxdepth = resultados_optimizacion$maxdepth[mejor_idx],
    minsplit = resultados_optimizacion$minsplit[mejor_idx],
    minbucket = resultados_optimizacion$minbucket[mejor_idx]
  )
)

# Mostrar mejores hiperparámetros en tabla
mejores_hiperparametros <- data.frame(
  Hiperparametro = c("maxdepth", "minsplit", "minbucket", "AUC en validación"),
  Valor = c(
    resultados_optimizacion$maxdepth[mejor_idx],
    resultados_optimizacion$minsplit[mejor_idx],
    resultados_optimizacion$minbucket[mejor_idx],
    round(resultados_optimizacion$auc_validacion[mejor_idx], 4)
  ),
  stringsAsFactors = FALSE
)

knitr::kable(mejores_hiperparametros,
  digits = 4,
  caption = "Mejores hiperparámetros encontrados en la optimización"
)
```

#### Visualización de la relación entre hiperparámetros y performance

A continuación se presentan cuatro visualizaciones que permiten analizar cómo los diferentes hiperparámetros afectan el rendimiento del modelo:

**Gráfico 1: Relación entre maxdepth y AUC**

Este gráfico muestra cómo cambia el rendimiento (AUC) según la profundidad máxima del árbol. Los puntos representan cada combinación específica de hiperparámetros, mientras que la línea azul suavizada muestra la tendencia general. Permite identificar si existe una profundidad óptima o si el rendimiento mejora indefinidamente.

```{r message=FALSE, warning=FALSE}
# Gráfico 1: AUC vs maxdepth
ggplot(resultados_optimizacion, aes(x = maxdepth, y = auc_validacion)) +
  geom_point(alpha = 0.6, size = 2) +
  geom_smooth(method = "loess", se = FALSE, color = "steelblue") +
  labs(
    title = "Relación entre maxdepth y AUC en validación",
    x = "Profundidad máxima del árbol",
    y = "AUC-ROC en validación"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

**Gráfico 2: Relación entre minsplit y AUC**

Este gráfico analiza cómo afecta el número mínimo de observaciones para hacer un split. Valores bajos de minsplit permiten splits más finos (árboles más complejos), mientras que valores altos producen árboles más simples y generalizables. Ayuda a encontrar el balance entre flexibilidad y robustez.

```{r message=FALSE, warning=FALSE}
# Gráfico 2: AUC vs minsplit
ggplot(resultados_optimizacion, aes(x = minsplit, y = auc_validacion)) +
  geom_point(alpha = 0.6, size = 2) +
  geom_smooth(method = "loess", se = FALSE, color = "darkred") +
  labs(
    title = "Relación entre minsplit y AUC en validación",
    x = "Mínimo de observaciones para split",
    y = "AUC-ROC en validación"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

**Gráfico 3: Heatmap de AUC por maxdepth y minsplit**

Este heatmap visualiza la interacción entre dos hiperparámetros clave. Los colores representan el AUC: rojo (bajo), amarillo (medio), verde (alto). Permite identificar regiones óptimas de combinaciones de hiperparámetros y detectar patrones de interacción.

```{r message=FALSE, warning=FALSE}
# Gráfico 3: Heatmap de AUC por maxdepth y minsplit
ggplot(resultados_optimizacion, aes(x = maxdepth, y = minsplit, fill = auc_validacion)) +
  geom_tile() +
  scale_fill_gradient2(
    low = "red",
    mid = "yellow",
    high = "green",
    midpoint = median(resultados_optimizacion$auc_validacion)
  ) +
  labs(
    title = "Heatmap: AUC por maxdepth y minsplit",
    x = "maxdepth",
    y = "minsplit",
    fill = "AUC-ROC"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

#### Evaluación del árbol optimizado en el conjunto de testeo

```{r message=FALSE, warning=FALSE}
# Evaluar árbol optimizado en testeo
auc_test_optimizado <- evaluar_auc(mejor_arbol, testeo)

# Comparar con árbol básico
auc_test_basico <- evaluar_auc(arbol_basico, testeo)

# Tabla comparativa
comparacion_auc <- data.frame(
  Modelo = c("Árbol básico", "Árbol optimizado"),
  AUC_Test = c(auc_test_basico, auc_test_optimizado),
  Mejora = c(0, auc_test_optimizado - auc_test_basico),
  stringsAsFactors = FALSE
)

knitr::kable(comparacion_auc,
  digits = 4,
  caption = "Comparación de AUC-ROC en conjunto de testeo"
)

# Calcular métricas adicionales para el árbol optimizado
pred_class_opt <- predict(mejor_arbol, newdata = testeo, type = "class")
pred_class_opt <- factor(pred_class_opt, levels = levels(testeo$y))

matriz_conf_opt <- caret::confusionMatrix(pred_class_opt, testeo$y, positive = "yes")

# Métricas del árbol optimizado
metricas_optimizado <- data.frame(
  Metrica = c("Accuracy", "Precision", "Recall", "F1"),
  Valor = c(
    as.numeric(matriz_conf_opt$overall["Accuracy"]),
    as.numeric(matriz_conf_opt$byClass["Precision"]),
    as.numeric(matriz_conf_opt$byClass["Recall"]),
    as.numeric(matriz_conf_opt$byClass["F1"])
  ),
  stringsAsFactors = FALSE
)

knitr::kable(metricas_optimizado,
  digits = 4,
  caption = "Métricas del árbol optimizado en testeo"
)
```

#### Visualización del árbol optimizado

```{r message=FALSE, warning=FALSE}
# Visualizar el árbol optimizado
rpart.plot(mejor_arbol,
  type = 2,
  extra = 106,
  box.palette = "GnBu",
  main = "Árbol de decisión optimizado"
)

# Curva ROC comparativa
roc_basico <- pROC::roc(testeo$y,
  predict(arbol_basico, newdata = testeo, type = "prob")[, "yes"],
  levels = c("no", "yes"),
  direction = "<"
)

roc_optimizado <- pROC::roc(testeo$y,
  predict(mejor_arbol, newdata = testeo, type = "prob")[, "yes"],
  levels = c("no", "yes"),
  direction = "<"
)

# Datos para ggplot
roc_data <- data.frame(
  FPR = c(
    1 - coords(roc_basico, ret = "all")$specificity,
    1 - coords(roc_optimizado, ret = "all")$specificity
  ),
  TPR = c(
    coords(roc_basico, ret = "all")$sensitivity,
    coords(roc_optimizado, ret = "all")$sensitivity
  ),
  Modelo = c(
    rep("Árbol básico", length(coords(roc_basico, ret = "all")$specificity)),
    rep("Árbol optimizado", length(coords(roc_optimizado, ret = "all")$specificity))
  )
)

# Gráfico comparativo de curvas ROC
ggplot(roc_data, aes(x = FPR, y = TPR, color = Modelo)) +
  geom_line() +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray") +
  labs(
    title = "Comparación de curvas ROC: Árbol básico vs Optimizado",
    x = "1 - Especificidad (FPR)",
    y = "Sensibilidad (TPR)",
    caption = paste(
      "Básico AUC =", round(auc_test_basico, 4),
      "| Optimizado AUC =", round(auc_test_optimizado, 4)
    )
  ) +
  scale_color_manual(values = c("Árbol básico" = "red", "Árbol optimizado" = "blue")) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5),
    plot.caption = element_text(hjust = 0.5, face = "bold")
  )
```

#### Análisis de la optimización

La búsqueda exhaustiva de hiperparámetros ha permitido identificar una configuración óptima que mejora significativamente el rendimiento del modelo. Los resultados muestran que:

1. **Mejora en AUC**: El árbol optimizado alcanza un AUC de `r round(auc_test_optimizado, 4)` en el conjunto de testeo, comparado con `r round(auc_test_basico, 4)` del árbol básico.

2. **Hiperparámetros óptimos**: La combinación de `maxdepth = `r resultados_optimizacion$maxdepth[mejor_idx]`, `minsplit = `r resultados_optimizacion$minsplit[mejor_idx]`, y `minbucket = `r resultados_optimizacion$minbucket[mejor_idx]` produce el mejor rendimiento.

3. **Patrones observados**: 
   - Los valores moderados de `maxdepth` (entre 4-8) tienden a producir mejores resultados
   - Valores intermedios de `minsplit` (20-100) ofrecen un buen balance entre flexibilidad y generalización
   - `minbucket` debe ser proporcional a `minsplit` para evitar hojas muy pequeñas

4. **Robustez**: El modelo optimizado mantiene un buen balance entre sesgo y varianza, evitando tanto el sobreajuste como el subajuste.

## Sección 5: Interpretación de resultados

Al evaluar el árbol de decisión básico en el conjunto de testeo se observa que el modelo alcanza un **accuracy de aproximadamente 92%**, un valor elevado y aparentemente satisfactorio a primera vista. Sin embargo, al analizar con mayor detalle la matriz de confusión se advierte que este buen desempeño global está muy influido por el **desbalance de clases**: la mayoría de los clientes pertenece a la clase negativa (“no”), por lo que un modelo que siempre predijera “no” alcanzaría un **No Information Rate (NIR) del 90%**. Esto significa que la mejora real sobre una estrategia trivial es más reducida de lo que sugiere el valor bruto del accuracy, e incluso la prueba de significancia muestra que la diferencia respecto al NIR no es concluyente al 5% de significancia.

La interpretación cambia si se examinan otras métricas específicas de la clase positiva. La **precisión (precision)** alcanza un 64%, lo que indica que cuando el modelo se arriesga a predecir que un cliente aceptará el depósito (“yes”), tiene más de la mitad de probabilidades de estar en lo cierto. Sin embargo, la **sensibilidad (recall)** es mucho más baja, alrededor de 41%, lo que significa que el árbol deja escapar a más de la mitad de los clientes que efectivamente aceptan la oferta. Esto se traduce en un número importante de falsos negativos (39 en el testeo), lo que podría ser problemático en un contexto de marketing donde detectar a los posibles clientes interesados suele ser más importante que evitar falsos positivos. El **F1-score**, que combina precisión y recall en una sola medida, queda en torno a 0.50, lo que confirma un equilibrio modesto y refleja la dificultad del modelo para captar de manera consistente la clase minoritaria.

En contraste, la **especificidad del modelo es muy alta (97.5%)**, lo cual significa que el árbol distingue muy bien a los clientes que no contratarán el depósito, cometiendo muy pocos falsos positivos. Esta asimetría entre sensibilidad y especificidad está en línea con lo que se observa en la prueba de McNemar, que arroja un p-valor significativo indicando que los errores no se distribuyen de manera balanceada: el modelo tiende claramente a equivocarse más en los positivos que en los negativos.

Finalmente, la **curva ROC y el AUC confirman la existencia de señal predictiva**. El área bajo la curva se ubica en 0.79, lo que indica que, si se toma al azar un cliente que aceptó y uno que rechazó la oferta, en casi ocho de cada diez casos el modelo asigna una probabilidad mayor al cliente positivo que al negativo. Este valor refleja que, más allá del umbral de decisión fijo que usa `rpart` (basado en la mayoría de la hoja, equivalente a 0.5), las probabilidades generadas contienen información útil y podrían aprovecharse para mejorar el recall ajustando el umbral de clasificación.

## Sección 6: Análisis del impacto de etiquetas corruptas

En esta sección vamos a analizar cómo la presencia de etiquetas corruptas en el conjunto de entrenamiento afecta el rendimiento del modelo de árbol de decisión. Para esto, **reutilizaremos las funciones de optimización desarrolladas en la Sección 4**, específicamente:

- **`evaluar_auc`**: Para evaluar modelos usando AUC-ROC
- **`grid_hiperparametros`**: El mismo grid de hiperparámetros optimizado
- **`resultados_optimizacion`**: La estructura de resultados ya establecida

### Generación de conjuntos con etiquetas corruptas

```{r message=FALSE, warning=FALSE}
# Función auxiliar: invierte etiquetas binarias (no/yes) en una fracción dada
flip_labels <- function(df, prop) {
  stopifnot("y" %in% names(df))
  y <- df$y
  if (!is.factor(y)) y <- factor(y)
  if (all(c("no", "yes") %in% levels(y))) {
    y <- factor(y, levels = c("no", "yes"))
  }
  n <- nrow(df)
  n_flip <- floor(n * prop)
  if (n_flip <= 0) {
    return(list(data = df, flipped_idx = integer(0)))
  }
  idx <- sample(seq_len(n), size = n_flip, replace = FALSE)
  y_int <- as.integer(y) # no -> 1, yes -> 2
  y_int[idx] <- 3 - y_int[idx] # 1 <-> 2
  y_new <- factor(levels(y)[y_int], levels = levels(y))
  df$y <- y_new
  list(data = df, flipped_idx = idx)
}

# Semillas para reproducibilidad de las 6 versiones
set.seed(20250819)

# Asegurar existencia del conjunto de entrenamiento y conservar copia intacta
stopifnot(exists("entrenamiento"))
entrenamiento_original <- entrenamiento

# Generación de versiones con porcentajes solicitados (5%, 10%, 15%, 20%, 25%, 30%)
porcentajes_corrupcion <- c(0.05, 0.10, 0.15, 0.20, 0.25, 0.30)
versiones_corruptas <- list()

for (i in seq_along(porcentajes_corrupcion)) {
  prop <- porcentajes_corrupcion[i]
  version <- flip_labels(entrenamiento_original, prop)
  versiones_corruptas[[paste0("V", i)]] <- list(
    data = version$data,
    porcentaje = prop * 100,
    flipped_idx = version$flipped_idx
  )
}
```

### Reutilización de funciones de optimización de la Sección 4

Ahora aplicaremos el mismo proceso de optimización que desarrollamos en la **Sección 4** para cada versión corrupta:

```{r message=FALSE, warning=FALSE}
# Asegurar factores consistentes de 'y' en todos los conjuntos
for (nm in c("entrenamiento_original", "validacion", "testeo")) {
  if (exists(nm)) {
    tmp <- get(nm)
    if (!is.factor(tmp$y)) tmp$y <- factor(tmp$y)
    if (all(c("no", "yes") %in% levels(tmp$y))) {
      tmp$y <- factor(tmp$y, levels = c("no", "yes"))
    }
    assign(nm, tmp)
  }
}

# Evaluación de baseline en testeo usando evaluar_auc
baseline_test_auc <- evaluar_auc(mejor_arbol, testeo)

# Optimización para cada versión corrupta usando el mismo enfoque de la Sección 4
resultados_versiones <- list()
mejores_modelos_versiones <- list()

for (nm in names(versiones_corruptas)) {
  # Aplicar la misma optimización de hiperparámetros de la Sección 4
  resultados_temp <- data.frame(
    maxdepth = grid_hiperparametros$maxdepth,
    minsplit = grid_hiperparametros$minsplit,
    minbucket = grid_hiperparametros$minbucket,
    auc_validacion = NA_real_,
    stringsAsFactors = FALSE
  )

  # Búsqueda exhaustiva para esta versión corrupta usando evaluar_auc
  for (i in 1:nrow(grid_hiperparametros)) {
    control <- rpart.control(
      cp = 0,
      xval = 0,
      maxdepth = grid_hiperparametros$maxdepth[i],
      minsplit = grid_hiperparametros$minsplit[i],
      minbucket = grid_hiperparametros$minbucket[i]
    )

    modelo_temp <- rpart(
      y ~ .,
      data = versiones_corruptas[[nm]]$data,
      method = "class",
      control = control
    )

    resultados_temp$auc_validacion[i] <- evaluar_auc(modelo_temp, validacion)
  }

  # Encontrar mejor modelo para esta versión
  mejor_idx_temp <- which.max(resultados_temp$auc_validacion)
  mejor_modelo_temp <- rpart(
    y ~ .,
    data = versiones_corruptas[[nm]]$data,
    method = "class",
    control = rpart.control(
      cp = 0,
      xval = 0,
      maxdepth = resultados_temp$maxdepth[mejor_idx_temp],
      minsplit = resultados_temp$minsplit[mejor_idx_temp],
      minbucket = resultados_temp$minbucket[mejor_idx_temp]
    )
  )

  # Guardar resultados
  resultados_versiones[[nm]] <- resultados_temp
  mejores_modelos_versiones[[nm]] <- mejor_modelo_temp
}

# Evaluación comparativa en testeo
resultados_test_comparativo <- data.frame(
  Version = c("Baseline (0%)", paste0(" (", porcentajes_corrupcion * 100, "%)")),
  AUC_Test = c(baseline_test_auc, sapply(mejores_modelos_versiones, function(m) evaluar_auc(m, testeo))),
  Porcentaje_Corrupcion = c(0, porcentajes_corrupcion * 100),
  stringsAsFactors = FALSE
)

# Agregar métricas adicionales para análisis completo
for (i in 1:nrow(resultados_test_comparativo)) {
  if (i == 1) {
    modelo_actual <- mejor_arbol
  } else {
    modelo_actual <- mejores_modelos_versiones[[paste0("V", i - 1)]]
  }

  # Calcular métricas usando el mismo enfoque de la Sección 4
  pred_class <- predict(modelo_actual, newdata = testeo, type = "class")
  pred_class <- factor(pred_class, levels = levels(testeo$y))

  matriz_conf <- caret::confusionMatrix(pred_class, testeo$y, positive = "yes")

  resultados_test_comparativo$Accuracy[i] <- as.numeric(matriz_conf$overall["Accuracy"])
  resultados_test_comparativo$Precision[i] <- as.numeric(matriz_conf$byClass["Precision"])
  resultados_test_comparativo$Recall[i] <- as.numeric(matriz_conf$byClass["Recall"])
  resultados_test_comparativo$F1[i] <- as.numeric(matriz_conf$byClass["F1"])
}
# Mostrar resultados comparativos
knitr::kable(resultados_test_comparativo,
  digits = 4,
  caption = "Rendimiento comparativo: Baseline vs Versiones con etiquetas corruptas"
)
```
### Análisis de la evolución de hiperparámetros óptimos

Ahora analizaremos cómo cambian los valores de los hiperparámetros óptimos a medida que aumenta el porcentaje de etiquetas corruptas:

```{r message=FALSE, warning=FALSE}
# Extraer hiperparámetros óptimos para cada versión
hiperparametros_optimos <- data.frame(
  Porcentaje_Corrupcion = c(0, porcentajes_corrupcion * 100),
  maxdepth = NA_integer_,
  minsplit = NA_integer_,
  minbucket = NA_integer_,
  AUC_Validacion = NA_real_,
  stringsAsFactors = FALSE
)

# Para el baseline (0% corrupción)
hiperparametros_optimos$maxdepth[1] <- resultados_optimizacion$maxdepth[which.max(resultados_optimizacion$auc_validacion)]
hiperparametros_optimos$minsplit[1] <- resultados_optimizacion$minsplit[which.max(resultados_optimizacion$auc_validacion)]
hiperparametros_optimos$minbucket[1] <- resultados_optimizacion$minbucket[which.max(resultados_optimizacion$auc_validacion)]
hiperparametros_optimos$AUC_Validacion[1] <- max(resultados_optimizacion$auc_validacion)

# Para cada versión corrupta
for (i in seq_along(versiones_corruptas)) {
  nm <- paste0("V", i)
  mejor_idx <- which.max(resultados_versiones[[nm]]$auc_validacion)

  hiperparametros_optimos$maxdepth[i + 1] <- resultados_versiones[[nm]]$maxdepth[mejor_idx]
  hiperparametros_optimos$minsplit[i + 1] <- resultados_versiones[[nm]]$minsplit[mejor_idx]
  hiperparametros_optimos$minbucket[i + 1] <- resultados_versiones[[nm]]$minbucket[mejor_idx]
  hiperparametros_optimos$AUC_Validacion[i + 1] <- resultados_versiones[[nm]]$auc_validacion[mejor_idx]
}

# Mostrar tabla de hiperparámetros óptimos
knitr::kable(hiperparametros_optimos,
  digits = 4,
  caption = "Evolución de hiperparámetros óptimos con etiquetas corruptas"
)
```

#### Análisis de patrones en hiperparámetros óptimos

```{r message=FALSE, warning=FALSE}
# Gráfico 3: Evolución de maxdepth óptimo
ggplot(hiperparametros_optimos, aes(x = Porcentaje_Corrupcion, y = maxdepth)) +
  geom_line(color = "steelblue", linewidth = 1.2) +
  geom_point(size = 3, color = "darkred") +
  labs(
    title = "Evolución de maxdepth óptimo con etiquetas corruptas",
    x = "% de etiquetas corruptas",
    y = "maxdepth óptimo"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_y_continuous(breaks = unique(hiperparametros_optimos$maxdepth))

# Gráfico 4: Evolución de minsplit óptimo
ggplot(hiperparametros_optimos, aes(x = Porcentaje_Corrupcion, y = minsplit)) +
  geom_line(color = "darkgreen", linewidth = 1.2) +
  geom_point(size = 3, color = "darkred") +
  labs(
    title = "Evolución de minsplit óptimo con etiquetas corruptas",
    x = "% de etiquetas corruptas",
    y = "minsplit óptimo"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_y_continuous(breaks = unique(hiperparametros_optimos$minsplit))

# Gráfico 5: Evolución de minbucket óptimo
ggplot(hiperparametros_optimos, aes(x = Porcentaje_Corrupcion, y = minbucket)) +
  geom_line(color = "purple", linewidth = 1.2) +
  geom_point(size = 3, color = "darkred") +
  labs(
    title = "Evolución de minbucket óptimo con etiquetas corruptas",
    x = "% de etiquetas corruptas",
    y = "minbucket óptimo"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_y_continuous(breaks = unique(hiperparametros_optimos$minbucket))
``` 

### Análisis de desempeño y cambios de hiperparámetros con etiquetas corruptas

El experimento muestra cómo el rendimiento del árbol de decisión se degrada al aumentar el porcentaje de etiquetas corruptas en el conjunto de entrenamiento.

Tendencia general:

Con 5-15% de etiquetas corruptas, la performance cae suavemente: la AUC baja de 0.87 (baseline) a valores cercanos a 0.84-0.85, con variaciones menores en F1 y Recall.

A partir del 20% aparece un punto de quiebre, con un descenso acelerado de la AUC (0.78) y pérdidas notorias en precisión.

El 30% de ruido marca un deterioro fuerte: la precisión se desploma (0.27) y el F1 cae al mínimo, aunque el Recall sube artificialmente a 0.50 porque el modelo clasifica más positivos de forma errónea.

En síntesis, la degradación no es lineal: existe un umbral crítico alrededor del 20-25% donde el modelo se vuelve poco confiable.

### Gráfico 1: AUC vs. Porcentaje de etiquetas corruptas
```{r}
library(ggplot2)

df_plot <- data.frame(
  Corrupcion = c(0, 5, 10, 15, 20, 25, 30),
  AUC = c(0.8714, 0.8597, 0.8366, 0.8449, 0.7826, 0.8327, 0.8156)
)

ggplot(df_plot, aes(x = Corrupcion, y = AUC)) +
  geom_line(color = "steelblue") +
  geom_point(linewidth = 3, color = "darkred") +
  labs(
    title = "AUC en test vs % de etiquetas corruptas",
    x = "% de etiquetas corruptas", y = "AUC"
  ) +
  theme_minimal()
```

#### Explicación: 
La curva es relativamente estable hasta 15%, luego se observa una caída brusca. Esto confirma el carácter no lineal de la degradación.

### Gráfico 2: Accuracy, Precision, Recall y F1

```{r}
df_plot2 <- data.frame(
  Corrupcion = rep(c(0, 5, 10, 15, 20, 25, 30), each = 4),
  Metrica = rep(c("Accuracy", "Precision", "Recall", "F1"), times = 7),
  Valor = c(
    0.9099, 0.5556, 0.3788, 0.4505,
    0.9055, 0.5208, 0.3788, 0.4386,
    0.9055, 0.5192, 0.4091, 0.4576,
    0.9010, 0.4909, 0.4091, 0.4463,
    0.8922, 0.4407, 0.3939, 0.4160,
    0.8892, 0.4262, 0.3939, 0.4094,
    0.8198, 0.2705, 0.5000, 0.3511
  )
)

ggplot(df_plot2, aes(x = Corrupcion, y = Valor, color = Metrica)) +
  geom_line() +
  geom_point(size = 2) +
  labs(
    title = "Evolución de métricas con etiquetas corruptas",
    x = "% de etiquetas corruptas", y = "Valor de la métrica"
  ) +
  theme_minimal()
```

#### Explicación:
- Accuracy y Precision caen progresivamente, con fuerte desplome al 30%.
- Recall se mantiene relativamente estable y sube en V6, pero es un aumento engañoso, fruto de más falsos positivos.
- F1 sigue la caída general y muestra el impacto acumulado.

#### Hiperparámetros

Con el análisis detallado de hiperparámetros óptimos implementado anteriormente, ahora podemos observar cómo cambian los valores de `maxdepth`, `minsplit` y `minbucket` a medida que aumenta el porcentaje de etiquetas corruptas. 

Los gráficos de evolución muestran si existe un patrón sistemático en la adaptación de la estructura del árbol al ruido en los datos.

Esto nos permite determinar si la robustez del modelo proviene de cambios adaptativos en la estructura del árbol o si se mantiene constante a pesar del deterioro en la calidad de los datos de entrenamiento.



## Conclusiones

El análisis realizado nos permitió evaluar de manera integral el desempeño de los árboles de decisión en un problema de clasificación binaria con un dataset de gran tamaño y características mixtas (variables numéricas y categóricas). A lo largo de las distintas etapas observamos lo siguiente:
- Árbol básico vs. árbol optimizado: el modelo inicial construido con parámetros por defecto mostró un rendimiento aceptable, pero al ajustar los hiperparámetros (maxdepth, minsplit y minbucket) se obtuvieron mejoras significativas en AUC-ROC y métricas asociadas. Esto confirma la importancia de una búsqueda sistemática de hiperparámetros en modelos de árboles.
- Interpretabilidad y variables relevantes: el árbol optimizado permitió identificar claramente las variables con mayor peso predictivo, lo cual constituye una de las principales ventajas de los árboles de decisión frente a otros modelos más complejos y de “caja negra”.
- Impacto de etiquetas corruptas: la simulación de ruido en la variable objetivo mostró que el rendimiento del modelo se degrada a medida que aumenta el porcentaje de observaciones mal etiquetadas. La caída no fue lineal: se detectaron puntos críticos a partir de niveles intermedios de corrupción (20-25%) donde la performance empeoró más drásticamente. Esto refleja la sensibilidad de los árboles a la calidad de las etiquetas y la importancia de contar con procesos de recolección y validación confiables de los datos.
- Reflexión general: los árboles de decisión resultan efectivos para este problema, al balancear interpretabilidad y capacidad predictiva. Sin embargo, su sensibilidad al ruido y a la selección de hiperparámetros sugiere que en escenarios reales podría ser beneficioso complementar este enfoque con métodos más robustos, como ensembles (Random Forest, Gradient Boosting) o técnicas de preprocesamiento orientadas a la detección y corrección de errores de etiquetado.
El trabajo puso en evidencia tanto las fortalezas como las limitaciones de los árboles de decisión, ofreciendo un panorama completo sobre su aplicabilidad y destacando posibles líneas de mejora en futuros análisis.